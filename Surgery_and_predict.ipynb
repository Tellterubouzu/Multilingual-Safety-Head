{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定したheadの[Query,Key,Valuse]をマスクまたはスケーリングしたモデルを保存するコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from lib.utils.custommodel import CustomLlamaModelForCausalLM\n",
    "from lib.utils.batch_inference import surgery\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "model_path = \"Llama-2-7b-chat-hf\"\n",
    "# model_path = \"Llama3.1-8B-Introduction\"\n",
    "# model_path = \"Llama3.2-3B-Introduction\"\n",
    "\n",
    "if(\"/\" in model_path):\n",
    "    model_name = model_path.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパラ\n",
    "* mask_type : [scale_mask / mean_mask]\n",
    "    スケールマスク :　ヘッドのq,k,vのいずれかの重みにscale_factorをかけて調整，論文では小さい値をかけた無効化してた\n",
    "    ミーンマスク   :　重みの平均を取る\n",
    "* scale_factor : scale_maskの時q,k,vのいずれかの重みと積をとる値\n",
    "* head_mask{} : (Head, Layer):mask_qkv = ['q','k','v']\n",
    "    (3,5):['q','k','v]のとき，3層目のhead5のq,k,v全てを調整する\n",
    "* name_prefix : 調整後のモデルの名前のprefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_qkv = ['q']\n",
    "scale_factor = 1e-10\n",
    "mask_type = \"scale_mask\"\n",
    "\n",
    "head_mask = {\n",
    "    # (Head, Layer): mask_qkv = ['q']\n",
    "    (3,4): ['q','k','v']\n",
    "    }\n",
    "name_prefix = \"run_test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLlamaModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surgery_path = f\"./models/{model_name}-{name_prefix}\"\n",
    "# storage_path\n",
    "surgery_model = surgery(model, \n",
    "                        tokenizer, \n",
    "                        head_mask, \n",
    "                        mask_type, \n",
    "                        scale_factor,\n",
    "                        surgery_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 調整したモデルでの推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_lllama_predict import Custom_llama\n",
    "model = Custom_llama(surgery_path,quant_type=4,use_sys_prompt=False,use_history=False)\n",
    "generation_config = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.95,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\": 2048\n",
    "}\n",
    "print(model.predict(\"あなたは誰ですか？\",generation_config))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットでモデルの検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.batch_inference import inference\n",
    "from transformers import AutoTokenizer\n",
    "from lib.utils.custommodel import CustomLlamaModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import accelerate\n",
    "model = CustomLlamaModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference関数\n",
    "##### ARGS\n",
    "* surgery_path :読み込むモデルのパス\n",
    "* data_path : 使うデータセットのパス\n",
    "* accelerator : バッチ推論用accelerator\n",
    "* generate_cfg :　モデルの推論用の引数\n",
    "* inference_cgf :　torchのDataLoaderに渡すconfig\n",
    "* seed :　再現性担保用のseed値．pythonとcudaとかのseedをまとめて設定\n",
    "* sample_times : データセットの中の何個まで推論するか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surgery_path = f\"./models/{model_name}-{name_prefix}\"\n",
    "data = \"jailbreakbench\"\n",
    "data_path = f\"./exp_data/{data}.csv\"\n",
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "generate_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"top_k\": 1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "inference_config = {\n",
    "    \"use_conv\": True,\n",
    "    \"store_path\": f\"./exp_res/{data}/\",\n",
    "    \"batch_size\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model_name=surgery_path,\n",
    "          data_path=data_path,\n",
    "          accelerator=accelerator,\n",
    "          generate_cfg=generate_config,\n",
    "          inference_cfg=inference_config,\n",
    "          seed=32,\n",
    "          sample_times=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
